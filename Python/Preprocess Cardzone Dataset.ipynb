{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Cardzone Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import from SQL__ <br>\n",
    "2 sample rule to indicate the fraud label\n",
    "    1. High Risk Countries based on currency code\n",
    "    2. Transaction that occured from midnight until 5am\n",
    "\n",
    "__Currency Code__ <br>\n",
    "971 - AFGHANISTAN <br>\n",
    "643 - RUSSIAN <br>\n",
    "586 - PAKISTAN\n",
    "\n",
    "__SQL Statement__ <br>\n",
    "UPDATE `cz_authtxn` <br>\n",
    "SET `AUTHTXN_FRAUD_CHECK` = CASE <br>\n",
    "&emsp; WHEN AUTHTXN_CURRENCY_CODE = 971 AND AUTHTXN_TRXN_TIME < 050000 THEN 'D' <br>\n",
    "&emsp; WHEN AUTHTXN_CURRENCY_CODE = 643 AND AUTHTXN_TRXN_TIME < 050000 THEN 'D' <br>\n",
    "&emsp; WHEN AUTHTXN_CURRENCY_CODE = 586 AND AUTHTXN_TRXN_TIME < 050000 THEN 'D' <br>\n",
    "&emsp; ELSE 'F' <br>\n",
    "END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory\n",
    "df = pd.read_csv('D:/Python Project/Credit Card Fraud Detection/cardzone dataset/cz_authtxn.csv', skipinitialspace=True)\n",
    "    \n",
    "# print number of records in the dataset\n",
    "print(\"Number of records:\" , len(df))\n",
    "print(\"Number of features:\" , len(df.columns))\n",
    "\n",
    "# check is there any null value in cells at columns\n",
    "print(\"Number of features that has empty cells\" , len(df.columns[df.isna().any()].tolist()))\n",
    "df.columns[df.isna().any()].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks how many fraud in this dataset\n",
    "print(df['AUTHTXN_FRAUD_CHECK'].value_counts())\n",
    "\n",
    "print('\\nFraud is {}% of our data.'.format(df['AUTHTXN_FRAUD_CHECK'].value_counts()['D'] / float(df['AUTHTXN_FRAUD_CHECK'].value_counts()['F'])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Class variables that has 0 value for Genuine transactions and 1 for Fraud\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.pie(df.AUTHTXN_FRAUD_CHECK.value_counts(),autopct='%1.1f%%', labels=['Legitimate','Fraud'], colors=['yellowgreen','r'])\n",
    "plt.axis('equal')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion = Imbalance. Learning of the data highly bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Deal with missing values\n",
    "There are several strategies to deal with missing data and there is no exact right answer\n",
    "- A value from another randomly selected record.\n",
    "- A mean, median or mode value for the column. (reduces variance in the dataset)\n",
    "    - Categorical NaNs for mode\n",
    "    - Numerical NaNs for mean\n",
    "    - If there are outliers in Numerical, try median (less sensitive to them)\n",
    "- Drop those record / column\n",
    "- A value estimated by another predictive model.\n",
    "- A distint constant value, such as 0 or -9999\n",
    "\n",
    "However, missing values does not necessarily means to missing information. <br>\n",
    "For example, if someone does not own a car, then of course it has no color resulting in missing value <br>\n",
    "If replace the missing value with some other value might leads to wrong result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns which has over 70% NaN values \n",
    "If a particular column has over 60% or 70% NaN values, it is better to drop it because it does not contributes towards giving information to ML model \n",
    "https://www.researchgate.net/publication/239608247_Machine_Learning_Based_Missing_Value_Imputation_Method_for_Clinical_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, print out the exact 30% records number that i want to keep the column\n",
    "print(\"Number of records that i want to keep in columns : \", len(df) * 0.3)\n",
    "\n",
    "df.isna().mean().round(4) * 100\n",
    "\n",
    "# Now, drop any columns that has 70% NaN\n",
    "df.dropna(thresh=0.3*len(df), axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print list of columns after initial drop\n",
    "print(\"The number of columns after initial dropping is \", len(df.columns))\n",
    "print(df.columns)\n",
    "\n",
    "# Recheck still got any left out blank cell in the columns that does not meet threshold\n",
    "print(\"\\nThe number of blank records in columns after initial dropping is \", len(df.columns[df.isna().any()]))\n",
    "df.columns[df.isna().any()].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns according to prototype design document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['AUTHTXN_NO', 'AUTHTXN_CARDHOLDER_NAME', 'AUTHTXN_SYSTEM_ID', 'AUTHTXN_APPROVED_AMT', 'AUTHTXN_STAN',\n",
    "                'AUTHTXN_PREV_STAN', 'AUTHTXN_TRANS_DATETIME', 'AUTHTXN_REQUEST_DATE', 'AUTHTXN_REQUEST_TIME', 'AUTHTXN_RESPONSE_DATE',\n",
    "                'AUTHTXN_RESPONSE_TIME', 'AUTHTXN_SETTLED_DATE', 'AUTHTXN_LAST_UPDATE_DATE', 'AUTHTXN_LAST_UPDATE_TIME', 'AUTHTXN_CARD_EXPIRY_DATE',\n",
    "                'AUTHTXN_POS_COND_CODE', 'AUTHTXN_RETRIEVAL_REFNO', 'AUTHTXN_OLD_RETRIEVAL_REFNO', 'AUTHTXN_APPROVAL_CODE', 'AUTHTXN_RESPONSE_CODE',\n",
    "                'AUTHTXN_MERCHANT_NAME', 'AUTHTXN_SETTLED_IND', 'AUTHTXN_AUTO_EXPIRY_DATE', 'AUTHTXN_SUBSIDY_REBATE_AMT', 'AUTHTXN_MERC_MDR_AMT',\n",
    "                'AUTHTXN_MERC_COMM_AMT', 'AUTHTXN_PROCESSEDBY', 'AUTHTXN_TYPE', 'AUTHTXN_INTERCHG_IND', 'AUTHTXN_GEOGRAPHY_IND',\n",
    "                'AUTHTXN_BONUS','AUTHTXN_FEE', 'AUTHTXN_ACQ_CHARGE_AT_IND', 'AUTHTXN_POST_IND', 'AUTHTXN_COMPONENT_ID',\n",
    "                'AUTHTXN_MTI', 'AUTHTXN_PROC_CD', 'AUTHTXN_BONUS_POINT', 'AUTHTXN_TERMBONUS_POINT', 'VERSION',\n",
    "                'AUTHTXN_SUBSIDY_REBATE_QTY', 'AUTHTXN_STMT_INC_BONUS', 'AUTHTXN_STMT_INC_FEE', 'AUTHTXN_STMT_INC_COST', 'AUTHTXN_STMT_INC_COMM',\n",
    "                'AUTHTXN_EBONUS', 'AUTHTXN_EFEE', 'AUTHTXN_SERVICE_CODE', 'AUTHTXN_ALT_RESPONSE_CODE', 'AUTHTXN_GST_AMT',\n",
    "                'AUTHTXN_MERC_GST_AMT', 'AUTHTXN_VTXNTYPGRP_ID', 'AUTHTXN_EDC_SETTLED_IND', 'AUTHTXN_VS_TRXN_ID', 'AUTHTXN_MATCH_PREVTXN_IND',\n",
    "                'AUTHTXN_ACQ_INST_ID', 'AUTHTXN_EDC_SETTLED_DATE', 'AUTHTXN_INTERBRANCH_IND', 'AUTHTXN_PIN_BASED', 'AUTHTXN_EXP_IND',\n",
    "                'AUTHTXN_FOREX_MARKUP_AMT', 'AUTHTXN_EXCESS_AMT']\n",
    "\n",
    "for col in drop_columns:\n",
    "    df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"The number of columns after second dropping is \", len(df.columns))\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nThe number of blank records in columns after second dropping is \", len(df.columns[df.isna().any()]))\n",
    "df.columns[df.isna().any()].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print in a graphical way to visualize\n",
    "import missingno as msno\n",
    "\n",
    "msno.bar(df.sample(3000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values count in columns\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns if only have one unique value\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        df.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After dropping columns which has only 1 unique values\")\n",
    "print(\"========================================================\")\n",
    "print(\"Number of records:\" , len(df))\n",
    "print(\"Number of features:\" , len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace 0 to the missing values in colum\n",
    "columns_nan_value = df.loc[:, df.isna().sum() > 0].columns\n",
    "\n",
    "for i, col in enumerate(columns_nan_value):\n",
    "    df[col].fillna(value = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck for any blank columns\n",
    "df.isna().sum() > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date and time column to readable DateTime object\n",
    "First, convert their types into String, then preprocess them as follows:\n",
    "- For Dates which dont have year, assume it as 2018\n",
    "- For Time(24-hour) which dont have Hours and Minutes infront, append the time with 00 : 00 : xx <br> Because of Excel automatically delete initial 0s infront, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.AUTHTXN_TRXN_DATE = df.AUTHTXN_TRXN_DATE.astype(str)\n",
    "df.AUTHTXN_TRXN_TIME = df.AUTHTXN_TRXN_TIME.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, date in enumerate(df.AUTHTXN_TRXN_DATE):\n",
    "    if len(date) == 3:\n",
    "        new_date = pd.datetime.strptime(date, '%m%d').date()\n",
    "        new_date = new_date.replace(2018, 1)\n",
    "        df.at[i, 'AUTHTXN_TRXN_DATE'] = new_date\n",
    "    elif len(date) == 4:\n",
    "        new_date = pd.datetime.strptime(date, '%m%d').date()\n",
    "        new_date = new_date.replace(2018)\n",
    "        df.at[i, 'AUTHTXN_TRXN_DATE'] = new_date\n",
    "    elif len(date) == 8:\n",
    "        new_date = pd.datetime.strptime(date, '%Y%m%d').date()\n",
    "        df.at[i, 'AUTHTXN_TRXN_DATE'] = new_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, time in enumerate(df.AUTHTXN_TRXN_TIME):\n",
    "    if len(time) == 1 or len(time) == 2:\n",
    "        new_time = pd.datetime.strptime(time, '%S').time()\n",
    "        df.at[i, 'AUTHTXN_TRXN_TIME'] = new_time\n",
    "    elif len(time) == 3:\n",
    "        time = str(0) + time\n",
    "        new_time = pd.datetime.strptime(time, '%M%S').time()\n",
    "        df.at[i, 'AUTHTXN_TRXN_TIME'] = new_time\n",
    "    elif len(time) == 4:\n",
    "        new_time = pd.datetime.strptime(time, '%M%S').time()\n",
    "        df.at[i, 'AUTHTXN_TRXN_TIME'] = new_time\n",
    "    elif len(time) == 5:\n",
    "        time = str(0) + time\n",
    "        new_time = pd.datetime.strptime(time, '%H%M%S').time()\n",
    "        df.at[i, 'AUTHTXN_TRXN_TIME'] = new_time\n",
    "    elif len(time) == 6:\n",
    "        new_time = pd.datetime.strptime(time, '%H%M%S').time()\n",
    "        df.at[i, 'AUTHTXN_TRXN_TIME'] = new_time\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df.AUTHTXN_TRXN_TIME, format='%H:%M:%S').dt.hour.value_counts().sort_index().plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Year\"] = pd.to_datetime(df.AUTHTXN_TRXN_DATE).dt.year\n",
    "df[\"Month\"] = pd.to_datetime(df.AUTHTXN_TRXN_DATE).dt.month\n",
    "df[\"Day\"] = pd.to_datetime(df.AUTHTXN_TRXN_DATE).dt.day\n",
    "df[\"Day_of_week\"] = pd.to_datetime(df.AUTHTXN_TRXN_DATE).dt.dayofweek\n",
    "\n",
    "df[\"Hour\"] = pd.to_datetime(df.AUTHTXN_TRXN_TIME, format='%H:%M:%S').dt.hour\n",
    "\n",
    "# Period of time labeling\n",
    "hours = df['Hour']\n",
    "bins = [-1, 4, 8, 16, 20]\n",
    "labels = ['Midnight', 'Morning','Afternoon','Evening','Night']\n",
    "df['Period_of_time']  = np.array(labels)[np.array(bins).searchsorted(hours)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['AUTHTXN_TRXN_DATE'], axis=1, inplace=True)\n",
    "df.drop(['AUTHTXN_TRXN_TIME'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move AUTHTXN_FRAUD_CHECK as labels to last index for easy see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, map the D as 1, F as 0\n",
    "df['AUTHTXN_FRAUD_CHECK'] = df['AUTHTXN_FRAUD_CHECK'].map({'F': 0, 'D': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_label_df = df.AUTHTXN_FRAUD_CHECK\n",
    "df.drop(['AUTHTXN_FRAUD_CHECK'], axis=1, inplace=True)\n",
    "df['AUTHTXN_FRAUD_CHECK'] = fraud_label_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col not in ['AUTHTXN_REQUEST_AMT', 'AUTHTXN_NET_AMT', 'AUTHTXN_BILLING_TXN_AMT']:\n",
    "        if(df[col].dtype == np.float64 or df[col].dtype == np.int64):\n",
    "            df[col] = df[col].astype(str)\n",
    "        elif(df[col].dtype == np.float32 or df[col].dtype == np.int32):\n",
    "            df[col] = df[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the pre-processed dataset and fraud set (AUTHTXN_FRAUD_CHECK as D only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\Python Project\\Credit Card Fraud Detection\\cardzone dataset\\preprocessed_dataset_3.csv', index=False)\n",
    "# df.loc[df['AUTHTXN_FRAUD_CHECK'] == 'D'].to_excel(r'D:\\Python Project\\Credit Card Fraud Detection\\cardzone dataset\\fraud_set.xlsx',\n",
    "#                                                 index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
