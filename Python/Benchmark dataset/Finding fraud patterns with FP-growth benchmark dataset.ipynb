{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding fraud patterns with FP-growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>N</th>\n",
       "      <th>K</th>\n",
       "      <th>E</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>K</td>\n",
       "      <td>E</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>K</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>C</td>\n",
       "      <td>K</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>K</td>\n",
       "      <td>I</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   M  O  N  K    E    Y\n",
       "0  D  O  N  K    E    Y\n",
       "1  M  A  K  E  NaN  NaN\n",
       "2  M  U  C  K    Y  NaN\n",
       "3  C  O  O  K    I    E"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory\n",
    "df = pd.read_csv('D:/Python Project/Credit Card Fraud Detection/benchmark dataset/Test FP-Growth.csv')\n",
    "\n",
    "# printing the first 5 columns for data visualization \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute FP-growth algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import environment path to pyspark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = r\"D:\\apache-spark\" # spark installed folder\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "sys.path.insert(0, spark_path + \"/bin\")\n",
    "sys.path.insert(0, spark_path + \"/python/pyspark/\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/py4j-0.10.7-src.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export csv to txt file\n",
    "df.to_csv('processed_itemsets.txt', index=None, sep=' ', mode='w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'O', 'N', 'K', 'E', 'Y']\n",
      "['D', 'O', 'N', 'K', 'E', 'Y']\n",
      "['M', 'A', 'K', 'E', '']\n",
      "['M', 'U', 'C', 'K', 'Y', '']\n",
      "['C', 'O', 'O', 'K', 'I', 'E']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# creating necessary variable\n",
    "new_itemsets_list = []\n",
    "skip_first_iteration = 1\n",
    "\n",
    "# find the duplicate item and add a counter at behind\n",
    "with open(\"processed_itemsets.txt\", 'r') as fp:\n",
    "    itemsets_list = csv.reader(fp, delimiter =' ', skipinitialspace=True) \n",
    "    for itemsets in itemsets_list:\n",
    "        unique_itemsets = []\n",
    "        counter = 2\n",
    "        for item in itemsets:\n",
    "            if itemsets.count(item) > 1:\n",
    "                \n",
    "                if skip_first_iteration == 1:\n",
    "                    unique_itemsets.append(item)\n",
    "                    skip_first_iteration = skip_first_iteration + 1\n",
    "                    continue\n",
    "                    \n",
    "                duplicate_item = item + \"__(\" + str(counter) + \")\"\n",
    "                unique_itemsets.append(duplicate_item)\n",
    "                counter = counter + 1\n",
    "            else:\n",
    "                unique_itemsets.append(item)\n",
    "        print(itemsets)\n",
    "        new_itemsets_list.append(unique_itemsets)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the new itemsets into file\n",
    "with open('processed_itemsets.txt', 'w+') as f:\n",
    "    for items in new_itemsets_list:\n",
    "        for item in items:\n",
    "            f.write(\"{} \".format(item))\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "# initialize spark\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile('processed_itemsets.txt').cache()\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__minSupport__: The minimum support for an itemset to be identified as frequent. <br>\n",
    "For example, if an item appears 3 out of 5 transactions, it has a support of 3/5=0.6.\n",
    "\n",
    "__minConfidence__: Minimum confidence for generating Association Rule. Confidence is an indication of how often an association rule has been found to be true. For example, if in the transactions itemset X appears 4 times, X and Y co-occur only 2 times, the confidence for the rule X => Y is then 2/4 = 0.5.\n",
    "\n",
    "__numPartitions__: The number of partitions used to distribute the work. By default the param is not set, and number of partitions of the input dataset is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FPGrowth.train(transactions, minSupport=0.6, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets : Item Support\n",
      "====================================\n",
      "['K'] : 5\n",
      "['E'] : 4\n",
      "['E', 'K'] : 4\n",
      "['M'] : 3\n",
      "['M', 'K'] : 3\n",
      "['O'] : 3\n",
      "['O', 'E'] : 3\n",
      "['O', 'E', 'K'] : 3\n",
      "['O', 'K'] : 3\n",
      "['Y'] : 3\n",
      "['Y', 'K'] : 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequent Itemsets : Item Support\")\n",
    "print(\"====================================\")\n",
    "for index, frequent_itemset in enumerate(result):\n",
    "    print(str(frequent_itemset.items) + ' : ' + str(frequent_itemset.freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = sorted(model._java_model.generateAssociationRules(0.8).collect(), key=lambda x: x.confidence(), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antecedent => Consequent : Min Confidence\n",
      "========================================\n",
      "{O} => {E}: 1.0\n",
      "{O} => {K}: 1.0\n",
      "{E} => {K}: 1.0\n",
      "{Y} => {K}: 1.0\n",
      "{M} => {K}: 1.0\n",
      "{O,E} => {K}: 1.0\n",
      "{O,K} => {E}: 1.0\n",
      "{K} => {E}: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Antecedent => Consequent : Min Confidence\")\n",
    "print(\"========================================\")\n",
    "for rule in rules[:200]:\n",
    "    print(rule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop spark session\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
